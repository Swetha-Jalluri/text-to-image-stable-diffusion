# Text-to-Image Generation System  
**Stable Diffusion with CLIP Conditioning**

Author: Neha Dharanu  
Date: December 2025  

---

## ğŸ“Œ Overview
This project implements a **text-to-image generation system** using a Stable Diffusion pipeline conditioned on CLIP text embeddings. The system converts natural language prompts into high-quality 512Ã—512 images and includes **evaluation, parameter sensitivity analysis, and reproducible testing**.

The project is designed to run **entirely on CPU**, making it portable and reproducible without requiring GPU access.

---

## ğŸš€ Key Features
- Text-to-image generation using Stable Diffusion
- CLIP-based text conditioning
- Fine-tuned local checkpoint loading
- CPU-only execution
- Parameter sensitivity analysis (CFG scale, inference steps, schedulers)
- Quantitative evaluation using **FID** and **Inception Score**
- Negative prompt analysis
- Clean Streamlit-based web demo
- Reproducible inference testing script

---

## ğŸ§  Model Architecture
- **Text Encoder:** CLIP
- **Diffusion Core:** UNet + Scheduler
- **Latent Decoder:** VAE
- **Schedulers Evaluated:** Euler, DDIM, PNDM

---

## ğŸ—‚ï¸ Project Structure
See the *Project Structure* section below for full details.

---

# 2ï¸âƒ£ PROJECT STRUCTURE (PASTE INTO TECHNICAL DOCUMENT)

Use this **exact block** under a section titled:

### **Project Structure**

```text
FINALPROJECT/
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_evaluated_model/
â”‚       â”œâ”€â”€ feature_extractor/
â”‚       â”œâ”€â”€ scheduler/
â”‚       â”œâ”€â”€ text_encoder/
â”‚       â”œâ”€â”€ tokenizer/
â”‚       â”œâ”€â”€ unet/
â”‚       â”œâ”€â”€ vae/
â”‚       â”œâ”€â”€ best_config.json
â”‚       â””â”€â”€ model_index.json
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ comparison_proper_metrics.png
â”‚   â”œâ”€â”€ metrics_analysis_proper.png
â”‚   â”œâ”€â”€ parameter_sensitivity_analysis.png
â”‚   â””â”€â”€ my_custom_generation.png
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_inference.py
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ Generative_Project.ipynb
â”œâ”€â”€ dataset.md
â”œâ”€â”€ readme.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ Technical_documentation.pdf

Explanation

models/: Fine-tuned Stable Diffusion checkpoint

outputs/: Evaluation results and example generations

tests/: Standalone inference validation script

app.py: Streamlit web application

Generative_Project.ipynb: Experiments and analysis notebook

dataset.md: Dataset description and usage

Technical_documentation.pdf: Full system documentation
---

## ğŸ§ª Testing
A standalone inference testing script is provided:

```bash
python tests/test_inference.py
This script:

Loads the fine-tuned model

Runs inference on CPU

Saves an output image

Verifies the pipeline works end-to-end

â±ï¸ Expected runtime (CPU):

~30â€“60 seconds per image

ğŸ“Š Evaluation & Analysis

The project includes:

Scheduler comparison grids

FID and Inception Score heatmaps

CFG scale sensitivity analysis

Inference steps vs quality/speed trade-off

Final production recommendations

All evaluation images are stored in the outputs/ folder.

ğŸ–¥ï¸ Web Demo

A Streamlit application is included:

streamlit run app.py


Features:

Prompt input

Example prompt gallery

Image generation with loading spinner

Downloadable output image

ğŸ“¦ Setup Instructions (CPU Only)
pip install -r requirements.txt


Then run:

streamlit run app.py

âš–ï¸ Ethical Considerations

Uses publicly available pretrained models

No personal data collected

Potential misuse documented in technical report

Users encouraged to use responsible prompts

ğŸ“„ Documentation

Technical_documentation.pdf

dataset.md

Inline code documentation

ğŸ“Œ Notes

This project is CPU-only by design

Results may take longer compared to GPU execution

All experiments are reproducible

ğŸ”— Resources

Hugging Face Diffusers

OpenAI CLIP

PyTorch

Â© 2025 Neha Dharanu